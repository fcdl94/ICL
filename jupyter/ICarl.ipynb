{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ICarl.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "b-3Zv3Bz7YI8"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "Q8hyKPmM_rs8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import copy\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "#from networks import *\n",
        "from iCIFAR import *\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cPOlUwVRVM0M",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "######### Modifiable Settings ##########\n",
        "batch_size = 128  # Batch size\n",
        "n = 5  # Set the depth of the architecture: n = 5 -> 32 layers (See He et al. paper)\n",
        "nb_cl = 10  # Classes per group\n",
        "nb_protos = 20  # Number of prototypes per class at the end: total protoset memory/ total number of classes\n",
        "epochs = 10  # Total number of epochs\n",
        "lr_old = 2.  # Initial learning rate\n",
        "lr_strat = [49, 63]  # Epochs where learning rate gets decreased\n",
        "lr_factor = 5.  # Learning rate decrease factor\n",
        "wght_decay = 0.00001  # Weight Decay\n",
        "nb_runs = 10  # Number of runs (random ordering of classes at each run)\n",
        "np.random.seed(42)  # Fix the random seed\n",
        "\n",
        "########################################\n",
        "\n",
        "device = 'cuda'\n",
        "\n",
        "\n",
        "def save_checkpoint(state, filename):\n",
        "    torch.save(state, filename)\n",
        "\n",
        "# dictionary=number of training samples per class\n",
        "dictionary_size = 500"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "b-3Zv3Bz7YI8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Network"
      ]
    },
    {
      "metadata": {
        "id": "kfgqUjVb3DyM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import math\n",
        "from torch.nn import init\n",
        "\n",
        "class DownsampleA(nn.Module):\n",
        "    def __init__(self, nIn, nOut, stride):\n",
        "        super(DownsampleA, self).__init__()\n",
        "        assert stride == 2\n",
        "        self.avg = nn.AvgPool2d(kernel_size=1, stride=stride)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.avg(x)\n",
        "        return torch.cat((x, x.mul(0)), 1)\n",
        "\n",
        "class ResNetBasicblock(nn.Module):\n",
        "    expansion = 1\n",
        "    \"\"\"\n",
        "    RexNet basicblock (https://github.com/facebook/fb.resnet.torch/blob/master/models/resnet.lua)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None, relu=True):\n",
        "        super(ResNetBasicblock, self).__init__()\n",
        "\n",
        "        self.conv_a = nn.Conv2d(inplanes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn_a = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.conv_b = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn_b = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.downsample = downsample\n",
        "        self.relu = relu\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        basicblock = self.conv_a(x)\n",
        "        basicblock = self.bn_a(basicblock)\n",
        "        basicblock = F.relu(basicblock, inplace=True)\n",
        "\n",
        "        basicblock = self.conv_b(basicblock)\n",
        "        basicblock = self.bn_b(basicblock)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        y = residual + basicblock\n",
        "\n",
        "        if self.relu:\n",
        "            y = F.relu(y)\n",
        "\n",
        "        return y\n",
        "\n",
        "\n",
        "class CifarResNet(nn.Module):\n",
        "    \"\"\"\n",
        "    ResNet optimized for the Cifar Dataset, as specified in\n",
        "    https://arxiv.org/abs/1512.03385.pdf\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, block=ResNetBasicblock, depth=32, num_classes=100, channels=3):\n",
        "        \"\"\" Constructor\n",
        "        Args:\n",
        "          depth: number of layers.\n",
        "          num_classes: number of classes\n",
        "          base_width: base width\n",
        "        \"\"\"\n",
        "        super(CifarResNet, self).__init__()\n",
        "\n",
        "        # Model type specifies number of layers for CIFAR-10 and CIFAR-100 model\n",
        "        assert (depth - 2) % 6 == 0, 'depth should be one of 20, 32, 44, 56, 110'\n",
        "        layer_blocks = (depth - 2) // 6\n",
        "\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        self.conv_1_3x3 = nn.Conv2d(channels, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn_1 = nn.BatchNorm2d(16)\n",
        "\n",
        "        self.inplanes = 16\n",
        "        self.stage_1 = self._make_layer(block, 16, layer_blocks, 1)\n",
        "        self.stage_2 = self._make_layer(block, 32, layer_blocks, 2)\n",
        "        self.stage_3 = self._make_layer(block, 64, layer_blocks, 2, last=True)\n",
        "        self.avgpool = nn.AvgPool2d(8)\n",
        "        self.linear = nn.Linear(64, num_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
        "                # m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                m.weight.data.normal_(0, math.sqrt(1. / 64.))\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1, last=False):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = DownsampleA(self.inplanes, planes * block.expansion, stride)\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        if last:\n",
        "            for i in range(1, blocks - 1):\n",
        "                layers.append(block(self.inplanes, planes))\n",
        "            layers.append(block(self.inplanes, planes, relu=False))\n",
        "\n",
        "        else:\n",
        "            for i in range(1, blocks):\n",
        "                layers.append(block(self.inplanes, planes))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.conv_1_3x3(x)\n",
        "        x = self.bn_1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.stage_1(x)\n",
        "        x = self.stage_2(x)\n",
        "        x = self.stage_3(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def update_means(self, x, y):\n",
        "        self.linear.update_means(x, y)\n",
        "\n",
        "    def predict(self, x):\n",
        "        out = self.linear(x)\n",
        "        return out\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rb-wdUVy7bdT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Training"
      ]
    },
    {
      "metadata": {
        "id": "syS0tQrhH8hj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def fit_incremental(lr):\n",
        "  \n",
        "  new_lr = lr\n",
        "  optimizer = optim.SGD(filter(lambda p: p.requires_grad, network.parameters()), lr=new_lr, momentum=0.9,\n",
        "                        weight_decay=wght_decay, nesterov=False)\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    network.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    # In each epoch, we do a full pass over the training data:\n",
        "    train_err = 0\n",
        "    train_batches = 0\n",
        "    start_time = time.time()\n",
        "    count = 0\n",
        "    for inputs, targets_prep in icifar.minibatches(augment=True):\n",
        "        \n",
        "        targets = np.zeros((inputs.shape[0], 100), np.float32)  # 100 = classes of cifar\n",
        "        targets[range(len(targets_prep)), targets_prep.type(torch.int32)] = 1.  # prepare target for CE loss\n",
        "\n",
        "        inputs = inputs.to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        outputs = network.forward(inputs)  # feature vector only\n",
        "        prediction = network.predict(outputs)  # make the prediction with sigmoid, making g_y(xi)\n",
        "        targets = torch.tensor(targets).to(outputs.device)\n",
        "        targets_prep = torch.LongTensor(targets_prep).to(outputs.device)\n",
        "\n",
        "        if iteration > 0:  # apply distillation\n",
        "            outputs_old = network2.forward(inputs)\n",
        "            prediction_old = network2.predict(outputs_old)\n",
        "            targets[:, np.array(icifar.order[range(0, iteration * nb_cl)])] = \\\n",
        "                F.sigmoid(prediction_old[:, np.array(icifar.order[range(0, iteration * nb_cl)])])\n",
        "\n",
        "        loss_bx = loss(prediction, targets)  # joins classification and distillation losses\n",
        "        loss_bx.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss_bx.item()\n",
        "        _, predicted = prediction.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets_prep).sum().item()\n",
        "\n",
        "    # END loop minibatches\n",
        "    network.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    # count = 0\n",
        "    for inputs, targets_prep in icifar.minibatches(train=False):\n",
        "        # count += 1\n",
        "\n",
        "        targets = np.zeros((inputs.shape[0], 100), np.float32)\n",
        "        targets[range(len(targets_prep)), targets_prep.type(torch.int32)] = 1.\n",
        "\n",
        "        inputs = inputs.to(device)\n",
        "\n",
        "        outputs = network.forward(inputs)   # make the embedding\n",
        "        outputs = network.predict(outputs)  # make the NCM#\n",
        "\n",
        "        targets = torch.tensor(targets).to(outputs.device)\n",
        "        loss_bx = loss(outputs, targets)\n",
        "        test_loss += loss_bx.item()\n",
        "\n",
        "        targets_prep = torch.LongTensor(targets_prep).to(outputs.device)\n",
        "        _, predicted = outputs.max(1)\n",
        "        correct += predicted.eq(targets_prep).sum().item()\n",
        "\n",
        "        total += targets.size(0)\n",
        "\n",
        "    acc = 100. * correct / total\n",
        "    print(f\"Epoch {epoch} : Loss {test_loss/total:.8f} - Accuracy {acc:.2f}\")\n",
        "    \n",
        "      # adjust learning rate\n",
        "    if (epoch + 1) in lr_strat:\n",
        "      new_lr = new_lr / lr_factor\n",
        "      print(\"New LR:\" + str(new_lr))\n",
        "      optimizer = optim.SGD(filter(lambda p: p.requires_grad, network.parameters()), lr=new_lr, momentum=0.9,\n",
        "                            weight_decay=wght_decay, nesterov=False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2YWJC4gDFmu2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def update_exemplars(): \n",
        "    nb_protos_cl = int(np.ceil(nb_protos * 100. / nb_cl / (iteration + 1)))  # num of exemplars per class\n",
        "    # Herding\n",
        "    print('Updating exemplar set...')\n",
        "    network.eval()\n",
        "    \n",
        "    for iter_dico in range(nb_cl):\n",
        "        # Possible exemplars in the feature space and projected on the L2 sphere\n",
        "        # exemplars of class iter_dico + nb_cl\n",
        "        pinput = torch.tensor(icifar.get_X_of_class(icifar.order[iteration * nb_cl + iter_dico])).to(device)\n",
        "        mapped_prototypes = network.forward(pinput).cpu().detach().numpy()\n",
        "        D = mapped_prototypes.T\n",
        "        D = D / np.linalg.norm(D, axis=0)\n",
        "        # Herding procedure : ranking of the potential exemplars\n",
        "        mu = np.mean(D, axis=1)\n",
        "        # set exemplar to zero\n",
        "        alpha_dr_herding[iteration, :, iter_dico] = alpha_dr_herding[iteration, :, iter_dico] * 0\n",
        "        w_t = mu\n",
        "        iter_herding = 0\n",
        "        iter_herding_eff = 0\n",
        "        # Herding algorithm\n",
        "        while not (np.sum(alpha_dr_herding[iteration, :, iter_dico] != 0) == min(nb_protos_cl,\n",
        "                                                                                 500)) and iter_herding_eff < 1000:\n",
        "            tmp_t = np.dot(w_t, D)\n",
        "            ind_max = np.argmax(tmp_t)\n",
        "            iter_herding_eff += 1\n",
        "            if alpha_dr_herding[iteration, ind_max, iter_dico] == 0:\n",
        "                alpha_dr_herding[iteration, ind_max, iter_dico] = 1 + iter_herding\n",
        "                iter_herding += 1\n",
        "            w_t = w_t + mu - D[:, ind_max]\n",
        "\n",
        "    # Prepare the protoset\n",
        "    X_protoset_cumuls = []\n",
        "    Y_protoset_cumuls = []\n",
        "    \n",
        "    # Storing the selected exemplars in the protoset\n",
        "    for iteration2 in range(iteration + 1):\n",
        "\n",
        "        for iter_dico in range(nb_cl):\n",
        "            alph = alpha_dr_herding[iteration2, :, iter_dico]  # select the herd of the current class\n",
        "            alph = (alph > 0) * (alph < nb_protos_cl + 1) * 1. # put one in the ones to select\n",
        "            \n",
        "            # append exeplars in the protoset\n",
        "            X_protoset_cumuls.append(icifar.get_X_of_class(icifar.order[iteration2 * nb_cl + iter_dico])[np.where(alph == 1)[0]])\n",
        "            Y_protoset_cumuls.append(icifar.order[iteration2 * nb_cl + iter_dico] * np.ones(len(np.where(alph == 1)[0]), dtype=np.int32))\n",
        "\n",
        "    return X_protoset_cumuls, Y_protoset_cumuls\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dKNg9u9q774x",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def compute_means(iteration=10):\n",
        "  \n",
        "  class_means = np.zeros((64, 100, 2))\n",
        "  nb_protos_cl = int(np.ceil(nb_protos * 100. / nb_cl / (iteration)))  # num of exemplars per class\n",
        "  \n",
        "  for iteration2 in range(iteration):\n",
        "    current_cl = icifar.order[range(iteration2 * nb_cl, (iteration2 + 1) * nb_cl)]\n",
        "    for iter_dico in range(nb_cl):\n",
        "\n",
        "      pinput = torch.tensor(icifar.get_X_of_class(icifar.order[iteration2 * nb_cl + iter_dico])).to(\n",
        "          device)\n",
        "\n",
        "      # Collect data in the feature space for each class\n",
        "      mapped_prototypes = network.forward(pinput).cpu().detach().numpy()\n",
        "      D = mapped_prototypes.T\n",
        "      D = D / np.linalg.norm(D, axis=0)\n",
        "\n",
        "      # Flipped version also #todo non capisco perche' usa anche il flippato, check di performance se lo togliessi\n",
        "      inverted = np.array(icifar.get_X_of_class(icifar.order[iteration2 * nb_cl + iter_dico])[:, :, :, ::-1])\n",
        "      pinput2 = torch.tensor(np.array((inverted - icifar.pixel_means), dtype=np.float32)).to(device)\n",
        "      mapped_prototypes2 = network.forward(pinput2).cpu().detach().numpy()\n",
        "      D2 = mapped_prototypes2.T\n",
        "      D2 = D2 / np.linalg.norm(D2, axis=0)\n",
        "\n",
        "      # iCaRL\n",
        "      alph = alpha_dr_herding[iteration2, :, iter_dico] # importance of each image of this class\n",
        "      alph = (alph > 0) * (alph < nb_protos_cl + 1) * 1. # 1 if in the current herd\n",
        "      \n",
        "      alph = alph / np.sum(alph) # to make the average only for the current prototypes. \n",
        "      class_means[:, current_cl[iter_dico], 0] = (np.dot(D, alph) + np.dot(D2, alph)) / 2 #dot operation is for weighting each f(xi) with alpha\n",
        "      class_means[:, current_cl[iter_dico], 0] /= np.linalg.norm(class_means[:, current_cl[iter_dico], 0])\n",
        "\n",
        "      # Normal NCM\n",
        "      alph = np.ones(dictionary_size) / dictionary_size # to make the avg over all samples\n",
        "      class_means[:, current_cl[iter_dico], 1] = (np.dot(D, alph) + np.dot(D2, alph)) / 2 #dot operation is for weighting each f(xi) with alpha\n",
        "      class_means[:, current_cl[iter_dico], 1] /= np.linalg.norm(class_means[:, current_cl[iter_dico], 1])\n",
        "\n",
        "  np.save('cl_means', class_means)\n",
        "  return class_means\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "legmMTAalnt9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Validation"
      ]
    },
    {
      "metadata": {
        "id": "BBBx_1_Hjg6V",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from scipy.spatial.distance import cdist\n",
        "\n",
        "def test(network, iteration, class_means=None):\n",
        "  \n",
        "    if class_means is None:\n",
        "      class_means = compute_means(iteration)\n",
        "    \n",
        "    top1_acc_list = np.zeros(3)\n",
        "    \n",
        "    stat_hb1 = []\n",
        "    stat_icarl = []\n",
        "    stat_ncm = []\n",
        "\n",
        "    # TODO converto to icifar.minibatches\n",
        "    for inputs, targets_prep in icifar.minibatches_for_test(iteration):\n",
        "        \n",
        "        inputs = inputs.to(device)\n",
        "        \n",
        "        # compute prediction\n",
        "        outputs = network.forward(inputs) #returns embeddings\n",
        "        pred = network.predict(outputs).cpu().detach().numpy() #return classes of Hybrid1\n",
        "        outputs = outputs.cpu().detach().numpy()\n",
        "\n",
        "        outputs = (outputs.T / np.linalg.norm(outputs.T, axis=0)).T # normalize output\n",
        "\n",
        "        # Compute score for iCaRL\n",
        "        sqd = cdist(class_means[:, :, 0].T, outputs, 'sqeuclidean') # Squared euclidean distance\n",
        "        score_icarl = (-sqd).T\n",
        "        # Compute score for NCM\n",
        "        sqd = cdist(class_means[:, :, 1].T, outputs, 'sqeuclidean') # Squared euclidean distance\n",
        "        score_ncm = (-sqd).T\n",
        "\n",
        "        # Compute the accuracy over the batch\n",
        "        targets_prep = targets_prep.numpy()\n",
        "        \n",
        "        stat_hb1 += ([ll in best for ll, best in zip(targets_prep, np.argsort(pred, axis=1)[:, -1:])])\n",
        "        stat_icarl += ([ll in best for ll, best in zip(targets_prep, np.argsort(score_icarl, axis=1)[:, -1:])])\n",
        "        stat_ncm += ([ll in best for ll, best in zip(targets_prep, np.argsort(score_ncm, axis=1)[:, -1:])])\n",
        "\n",
        "    top1_acc_list[0] = np.average(stat_icarl) * 100 # ICarl\n",
        "    top1_acc_list[1] = np.average(stat_hb1) * 100   # Hybrid 1\n",
        "    top1_acc_list[2] = np.average(stat_ncm) * 100   # NCM\n",
        "\n",
        "    return top1_acc_list\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YNfCDLzvlj3i",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Main.py"
      ]
    },
    {
      "metadata": {
        "id": "TZ_vjzXvA1gT",
        "colab_type": "code",
        "outputId": "2068178b-d357-4b44-a3af-e65939e693ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "# get the data\n",
        "icifar = ICIFAR('data',batch_size, nb_cl,'fixed_order.npy')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "k5RtZAu5YPyK",
        "colab_type": "code",
        "outputId": "2e2499be-3b1d-4f91-8acc-bf56ec7e7ef9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1720
        }
      },
      "cell_type": "code",
      "source": [
        "# define network\n",
        "network = CifarResNet().to(device)\n",
        "loss = nn.BCEWithLogitsLoss(size_average=True)\n",
        "\n",
        "icifar.set_run(0)\n",
        "\n",
        "lr = lr_old\n",
        "\n",
        "# --- Initialization of the variables for this run\n",
        "X_protoset_cumuls = []\n",
        "Y_protoset_cumuls = []\n",
        "\n",
        "acc_cumuls = [[], [], []]\n",
        "acc_original = [[], [], []]\n",
        "# 10, 500, 10 -> 100 (class) * 500 (img per class)\n",
        "# ordered queue for all images of the dataset\n",
        "alpha_dr_herding = np.zeros((100 // nb_cl, dictionary_size, nb_cl), np.float32)  \n",
        "\n",
        "for iteration in range(100 // nb_cl):\n",
        "      \n",
        "  # Add the stored exemplars to the training data\n",
        "  if iteration > 0:\n",
        "      X_protoset = np.concatenate(X_protoset_cumuls)\n",
        "      Y_protoset = np.concatenate(Y_protoset_cumuls)\n",
        "  else:\n",
        "      X_protoset = None\n",
        "      Y_protoset = None\n",
        "\n",
        "  # Prepare the training data for the current batch of classes\n",
        "  icifar.next_iteration(X_protoset, Y_protoset)\n",
        "\n",
        "  ## TRAIN THIS ITERATION ##\n",
        "  print('Batch of classes number {0} arrives ...'.format(iteration + 1))\n",
        "  \n",
        "  fit_incremental(lr) #train for N epochs (after each epoch validate)\n",
        "\n",
        "  # Duplicate current network to distillate info\n",
        "  network2 = copy.deepcopy(network)\n",
        "  network2.eval()\n",
        "\n",
        "  # Save the network\n",
        "\n",
        "  save_checkpoint({\n",
        "      'iteration': iteration,\n",
        "      'state_dict': network.state_dict()\n",
        "  }, \"iter_\" + str(iteration) + \"_checkpoint.pth.tar\")\n",
        "  ## END OF TRAINING FOR THIS ITERATION ##\n",
        "\n",
        "  ## UPDATE EXEMPLARS ##\n",
        "  X_protoset_cumuls, Y_protoset_cumuls = update_exemplars()\n",
        "  \n",
        "  acc_cum = test(network, iteration+1)\n",
        "  \n",
        "  print(\"Cumulative results\")\n",
        "  print(\"  top 1 accuracy iCaRL          :\\t\\t{:.2f} %\".format(acc_cum[0]))\n",
        "  print(\"  top 1 accuracy Hybrid 1       :\\t\\t{:.2f} %\".format(acc_cum[1]))\n",
        "  print(\"  top 1 accuracy NCM            :\\t\\t{:.2f} %\".format(acc_cum[2]))\n",
        "          \n",
        "  acc_base = test(network, 1)\n",
        "  \n",
        "  print(\"First batch results\")\n",
        "  print(\"  top 1 accuracy iCaRL          :\\t\\t{:.2f} %\".format(acc_base[0]))\n",
        "  print(\"  top 1 accuracy Hybrid 1       :\\t\\t{:.2f} %\".format(acc_base[1]))\n",
        "  print(\"  top 1 accuracy NCM            :\\t\\t{:.2f} %\".format(acc_base[2]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
            "  warnings.warn(warning.format(ret))\n",
            "/content/iCIFAR.py:142: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  yield torch.tensor(inp_exc), torch.tensor(y[excerpt])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Batch of classes number 1 arrives ...\n",
            "Epoch 0 : Loss 0.00022935 - Accuracy 31.58\n",
            "Epoch 1 : Loss 0.00019274 - Accuracy 42.41\n",
            "Epoch 2 : Loss 0.00024587 - Accuracy 36.83\n",
            "Epoch 3 : Loss 0.00020382 - Accuracy 42.41\n",
            "Epoch 4 : Loss 0.00018578 - Accuracy 47.99\n",
            "Epoch 5 : Loss 0.00016403 - Accuracy 53.57\n",
            "Epoch 6 : Loss 0.00015605 - Accuracy 56.25\n",
            "Epoch 7 : Loss 0.00015694 - Accuracy 57.03\n",
            "Epoch 8 : Loss 0.00013885 - Accuracy 64.73\n",
            "Epoch 9 : Loss 0.00015215 - Accuracy 59.71\n",
            "Updating exemplar set...\n",
            "Computing mean-of_exemplars and theoretical mean...\n",
            "Cumulative results\n",
            "  top 1 accuracy iCaRL          :\t\t6.36 %\n",
            "  top 1 accuracy Hybrid 1       :\t\t59.71 %\n",
            "  top 1 accuracy NCM            :\t\t6.25 %\n",
            "First batch results\n",
            "  top 1 accuracy iCaRL          :\t\t6.36 %\n",
            "  top 1 accuracy Hybrid 1       :\t\t59.71 %\n",
            "  top 1 accuracy NCM            :\t\t6.25 %\n",
            "Batch of classes number 2 arrives ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1332: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 : Loss 0.00027126 - Accuracy 22.32\n",
            "Epoch 1 : Loss 0.00023607 - Accuracy 38.39\n",
            "Epoch 2 : Loss 0.00021935 - Accuracy 47.99\n",
            "Epoch 3 : Loss 0.00022347 - Accuracy 47.66\n",
            "Epoch 4 : Loss 0.00020678 - Accuracy 54.69\n",
            "Epoch 5 : Loss 0.00020632 - Accuracy 54.58\n",
            "Epoch 6 : Loss 0.00020562 - Accuracy 57.70\n",
            "Epoch 7 : Loss 0.00020171 - Accuracy 59.26\n",
            "Epoch 8 : Loss 0.00020149 - Accuracy 59.26\n",
            "Epoch 9 : Loss 0.00019685 - Accuracy 63.28\n",
            "Updating exemplar set...\n",
            "Computing mean-of_exemplars and theoretical mean...\n",
            "Cumulative results\n",
            "  top 1 accuracy iCaRL          :\t\t1.61 %\n",
            "  top 1 accuracy Hybrid 1       :\t\t50.99 %\n",
            "  top 1 accuracy NCM            :\t\t1.51 %\n",
            "First batch results\n",
            "  top 1 accuracy iCaRL          :\t\t5.25 %\n",
            "  top 1 accuracy Hybrid 1       :\t\t38.06 %\n",
            "  top 1 accuracy NCM            :\t\t5.47 %\n",
            "Batch of classes number 3 arrives ...\n",
            "Epoch 0 : Loss 0.00033172 - Accuracy 14.17\n",
            "Epoch 1 : Loss 0.00034507 - Accuracy 18.42\n",
            "Epoch 2 : Loss 0.00033184 - Accuracy 23.44\n",
            "Epoch 3 : Loss 0.00031998 - Accuracy 28.24\n",
            "Epoch 4 : Loss 0.00031947 - Accuracy 30.13\n",
            "Epoch 5 : Loss 0.00029409 - Accuracy 33.59\n",
            "Epoch 6 : Loss 0.00030914 - Accuracy 34.49\n",
            "Epoch 7 : Loss 0.00029368 - Accuracy 39.96\n",
            "Epoch 8 : Loss 0.00028011 - Accuracy 42.19\n",
            "Epoch 9 : Loss 0.00028831 - Accuracy 40.07\n",
            "Updating exemplar set...\n",
            "Computing mean-of_exemplars and theoretical mean...\n",
            "Cumulative results\n",
            "  top 1 accuracy iCaRL          :\t\t1.26 %\n",
            "  top 1 accuracy Hybrid 1       :\t\t37.60 %\n",
            "  top 1 accuracy NCM            :\t\t1.12 %\n",
            "First batch results\n",
            "  top 1 accuracy iCaRL          :\t\t10.16 %\n",
            "  top 1 accuracy Hybrid 1       :\t\t29.80 %\n",
            "  top 1 accuracy NCM            :\t\t10.38 %\n",
            "Batch of classes number 4 arrives ...\n",
            "Epoch 0 : Loss 0.00040688 - Accuracy 22.66\n",
            "Epoch 1 : Loss 0.00036613 - Accuracy 27.01\n",
            "Epoch 2 : Loss 0.00036021 - Accuracy 36.38\n",
            "Epoch 3 : Loss 0.00034296 - Accuracy 39.17\n",
            "Epoch 4 : Loss 0.00034130 - Accuracy 40.74\n",
            "Epoch 5 : Loss 0.00033192 - Accuracy 42.97\n",
            "Epoch 6 : Loss 0.00032411 - Accuracy 52.68\n",
            "Epoch 7 : Loss 0.00031986 - Accuracy 52.46\n",
            "Epoch 8 : Loss 0.00032468 - Accuracy 54.35\n",
            "Epoch 9 : Loss 0.00031324 - Accuracy 59.15\n",
            "Updating exemplar set...\n",
            "Computing mean-of_exemplars and theoretical mean...\n",
            "Cumulative results\n",
            "  top 1 accuracy iCaRL          :\t\t1.61 %\n",
            "  top 1 accuracy Hybrid 1       :\t\t34.45 %\n",
            "  top 1 accuracy NCM            :\t\t1.61 %\n",
            "First batch results\n",
            "  top 1 accuracy iCaRL          :\t\t9.26 %\n",
            "  top 1 accuracy Hybrid 1       :\t\t22.54 %\n",
            "  top 1 accuracy NCM            :\t\t9.38 %\n",
            "Batch of classes number 5 arrives ...\n",
            "Epoch 0 : Loss 0.00041327 - Accuracy 24.00\n",
            "Epoch 1 : Loss 0.00039546 - Accuracy 32.81\n",
            "Epoch 2 : Loss 0.00039258 - Accuracy 37.72\n",
            "Epoch 3 : Loss 0.00038267 - Accuracy 42.19\n",
            "Epoch 4 : Loss 0.00039363 - Accuracy 41.29\n",
            "Epoch 5 : Loss 0.00038796 - Accuracy 45.76\n",
            "Epoch 6 : Loss 0.00039882 - Accuracy 43.64\n",
            "Epoch 7 : Loss 0.00038474 - Accuracy 44.98\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}